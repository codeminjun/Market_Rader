"""
Market Rader - ì£¼ì‹ ë‰´ìŠ¤ ë””ìŠ¤ì½”ë“œ ë´‡
ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from config.settings import settings, get_news_sources, get_youtube_channels
from src.utils.logger import logger
from src.utils.cache import cache
from src.utils.weekly_archive import weekly_archive

# Collectors
from src.collectors.base import ContentItem, ContentType
from src.collectors.news import (
    NaverFinanceNewsCollector,
    RSSNewsCollector,
    create_rss_collectors,
    InvestingNewsCollector,
)
from src.collectors.reports import NaverResearchCollector, SeekingAlphaCollector, MorningBriefCollector
from src.collectors.youtube import YouTubeChannelMonitor, transcript_extractor
from src.collectors.market import market_data_collector

# Analyzers
from src.analyzer import (
    news_summarizer,
    report_summarizer,
    video_summarizer,
    importance_scorer,
    morning_brief_summarizer,
    market_signal_analyzer,
    report_analyzer,
    market_briefing_generator,
)
from src.analyzer.briefing_validator import briefing_validator

# Discord
from src.discord import (
    discord_sender,
    create_news_header_embed,
    create_news_list_embeds,
    create_market_signal_embed,
    create_breaking_news_embed,
    create_reports_header_embed,
    create_reports_list_embed,
    create_youtube_header_embed,
    create_youtube_list_embed,
    create_morning_brief_embed,
    create_market_close_embed,
    create_closing_review_embed,
    create_morning_strategy_embed,
)
from src.discord.embeds.report_embed import create_reports_with_analysis_embeds


def validate_settings() -> bool:
    """ì„¤ì • ê²€ì¦"""
    errors = settings.validate()
    if errors:
        for error in errors:
            logger.error(f"Configuration error: {error}")
        return False
    return True


def collect_news() -> dict:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ (êµ­ë‚´/í•´ì™¸ ë¶„ë¦¬, ë³‘ë ¬ ì²˜ë¦¬)"""
    from src.utils.constants import get_priority_from_string

    logger.info("=== Collecting News (Parallel) ===")
    korean_news = []
    international_news = []

    # ìˆ˜ì§‘ íƒœìŠ¤í¬ ì •ì˜
    def collect_naver():
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤"""
        collector = NaverFinanceNewsCollector(categories=["stock", "economy"])
        items = collector.collect()
        for item in items:
            item.extra_data["region"] = "korean"
        return ("korean", items, "Naver Finance")

    def collect_investing():
        """ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ì¸ê¸° ë‰´ìŠ¤ (í˜„ì¬ ì°¨ë‹¨ë¨ - ë¹„í™œì„±í™”)"""
        # ì¸ë² ìŠ¤íŒ…ë‹·ì»´ì´ ë´‡ ì°¨ë‹¨ ì¤‘ì´ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        # TODO: ë‹¤ë¥¸ ì¸ê¸° ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œ ëŒ€ì²´ í•„ìš”
        return ("korean", [], "Investing.com (disabled)")

    def collect_rss(source: dict, region: str):
        """RSS ë‰´ìŠ¤ ìˆ˜ì§‘"""
        priority = get_priority_from_string(source.get("priority", "medium"))
        collector = RSSNewsCollector(
            name=source["name"],
            url=source["url"],
            priority=priority,
        )
        items = collector.collect()
        for item in items:
            item.extra_data["region"] = region
        return (region, items, source["name"])

    # RSS ì†ŒìŠ¤ ë¡œë“œ
    news_config = get_news_sources()
    korean_sources = news_config.get("news", {}).get("korean", [])
    intl_sources = news_config.get("news", {}).get("international", [])

    # ëª¨ë“  ìˆ˜ì§‘ íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = []

        # ê¸°ë³¸ ìˆ˜ì§‘ê¸°
        futures.append(executor.submit(collect_naver))
        futures.append(executor.submit(collect_investing))

        # RSS ìˆ˜ì§‘ê¸°ë“¤
        for source in korean_sources:
            if source.get("type") == "rss" and source.get("enabled", True):
                futures.append(executor.submit(collect_rss, source, "korean"))

        for source in intl_sources:
            if source.get("type") == "rss" and source.get("enabled", True):
                futures.append(executor.submit(collect_rss, source, "international"))

        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(futures):
            try:
                region, items, source_name = future.result()
                if region == "korean":
                    korean_news.extend(items)
                else:
                    international_news.extend(items)
                logger.info(f"{source_name}: {len(items)} items")
            except Exception as e:
                logger.error(f"News collection failed: {e}")

    # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
    def dedupe(news_list):
        seen_ids = set()
        unique = []
        for item in news_list:
            if item.id not in seen_ids and not cache.is_sent(item.id, "news"):
                seen_ids.add(item.id)
                unique.append(item)
        return unique

    korean_news = dedupe(korean_news)
    international_news = dedupe(international_news)

    logger.info(f"Korean news: {len(korean_news)}, International: {len(international_news)}")
    return {"korean": korean_news, "international": international_news}


def collect_reports(extract_pdf: bool = False) -> list[ContentItem]:
    """ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)

    Args:
        extract_pdf: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—¬ë¶€ (ì˜¤ì „ ìŠ¤ì¼€ì¤„ì—ì„œë§Œ True)
    """
    logger.info("=== Collecting Reports (Parallel) ===")
    all_reports = []

    def collect_naver_research():
        """ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜"""
        collector = NaverResearchCollector(
            categories=["invest", "company", "market"],
            extract_pdf=extract_pdf,
            max_pdf_extract=5,  # ìƒìœ„ 5ê°œë§Œ PDF ì¶”ì¶œ
        )
        return collector.collect()

    def collect_seeking_alpha():
        """Seeking Alpha"""
        collector = SeekingAlphaCollector()
        return collector.collect()

    # ë³‘ë ¬ ìˆ˜ì§‘
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = {
            executor.submit(collect_naver_research): "Naver Research",
            executor.submit(collect_seeking_alpha): "Seeking Alpha",
        }

        for future in as_completed(futures):
            source_name = futures[future]
            try:
                reports = future.result()
                all_reports.extend(reports)
                logger.info(f"{source_name}: {len(reports)} items")
            except Exception as e:
                logger.error(f"{source_name} collection failed: {e}")

    # ì¤‘ë³µ ì œê±°
    seen_ids = set()
    unique_reports = []
    for item in all_reports:
        if item.id not in seen_ids and not cache.is_sent(item.id, "reports"):
            seen_ids.add(item.id)
            unique_reports.append(item)

    logger.info(f"Total unique reports: {len(unique_reports)}")
    return unique_reports


def collect_youtube() -> dict:
    """ìœ íŠœë¸Œ ì˜ìƒ ìˆ˜ì§‘ (í•œêµ­/í•´ì™¸ ë¶„ë¦¬)"""
    logger.info("=== Collecting YouTube Videos ===")

    try:
        youtube_monitor = YouTubeChannelMonitor()
        videos = youtube_monitor.collect()

        # ì¤‘ë³µ ì œê±° (ì´ë¯¸ ì „ì†¡ëœ ì˜ìƒ ì œì™¸)
        korean_videos = [
            v for v in videos.get("korean", [])
            if not cache.is_sent(v.id, "youtube")
        ]
        intl_videos = [
            v for v in videos.get("international", [])
            if not cache.is_sent(v.id, "youtube")
        ]

        logger.info(f"Total new videos - Korean: {len(korean_videos)}, Intl: {len(intl_videos)}")
        return {"korean": korean_videos, "international": intl_videos}

    except Exception as e:
        logger.error(f"YouTube collection failed: {e}")
        return {"korean": [], "international": []}


def collect_morning_briefs() -> list[ContentItem]:
    """Morning Brief ìˆ˜ì§‘ (ì˜¤ì „ ìŠ¤ì¼€ì¤„ ì „ìš©)"""
    logger.info("=== Collecting Morning Briefs ===")

    try:
        collector = MorningBriefCollector(max_briefs=3)
        briefs = collector.collect()

        # ì¤‘ë³µ ì œê±°
        unique_briefs = [
            b for b in briefs
            if not cache.is_sent(b.id, "morning_brief")
        ]

        logger.info(f"Collected {len(unique_briefs)} Morning Briefs")
        return unique_briefs

    except Exception as e:
        logger.error(f"Morning Brief collection failed: {e}")
        return []


def analyze_content(
    news: dict,
    reports: list[ContentItem],
    videos: dict,
    morning_briefs: list[ContentItem] = None,
) -> dict:
    """ì½˜í…ì¸  ë¶„ì„ ë° ìš”ì•½"""
    logger.info("=== Analyzing Content ===")

    korean_news = news.get("korean", [])
    intl_news = news.get("international", [])
    korean_videos = videos.get("korean", [])
    intl_videos = videos.get("international", [])
    morning_briefs = morning_briefs or []

    result = {
        "korean_news": korean_news,
        "international_news": intl_news,
        "news_summary": None,
        "market_signal": None,
        "breaking_news": [],
        "sector_news": {},
        "reports": reports,
        "reports_summary": None,
        "korean_videos": korean_videos,
        "international_videos": intl_videos,
        "video_summaries": {},
        "morning_briefs": morning_briefs,
        "morning_brief_summary": None,
    }

    # 0. Morning Brief ìš”ì•½ (ìˆëŠ” ê²½ìš°)
    if morning_briefs:
        try:
            result["morning_brief_summary"] = morning_brief_summarizer.summarize_multiple_briefs(
                morning_briefs
            )
            logger.info(f"Morning Brief summary generated from {len(morning_briefs)} briefs")
        except Exception as e:
            logger.warning(f"Morning Brief summarization failed: {e}")

    # 1. êµ­ë‚´ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
    if korean_news:
        scored = importance_scorer.filter_by_importance(korean_news, min_score=0.3)
        result["korean_news"] = scored[:settings.MAX_NEWS_COUNT]

    # 2. í•´ì™¸ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
    if intl_news:
        scored = importance_scorer.filter_by_importance(intl_news, min_score=0.3)
        result["international_news"] = scored[:settings.MAX_NEWS_COUNT]

    # 3. AI ìš”ì•½ (êµ­ë‚´ + í•´ì™¸ í•©ì³ì„œ)
    all_news = result["korean_news"] + result["international_news"]
    if all_news:
        try:
            result["news_summary"] = news_summarizer.summarize_news_batch(all_news[:15])
        except Exception as e:
            logger.warning(f"News summarization failed: {e}")

        # 3-1. ì„¹í„° ETF ì‹œì„¸ ìˆ˜ì§‘
        sector_etf_data = {}
        try:
            sector_etf_data = market_data_collector.collect_sector_etfs()
            if sector_etf_data:
                logger.info(f"Collected {len(sector_etf_data)} sector ETF prices")
                result["sector_etf_data"] = sector_etf_data
        except Exception as e:
            logger.warning(f"Sector ETF collection failed: {e}")

        # 3-1-1. ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ (ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥/í™˜ìœ¨ - ì£¼ê°„ ì•„ì¹´ì´ë¸Œìš©)
        try:
            market_data = market_data_collector.collect()
            if market_data and (market_data.kospi or market_data.kosdaq):
                result["market_data"] = market_data
                logger.info("Collected market index data for archive")
        except Exception as e:
            logger.warning(f"Market index collection failed: {e}")

        # 3-2. ì‹œì¥ ì‹œê·¸ë„ ë¶„ì„ (AI + ETF ì‹œì„¸)
        try:
            result["market_signal"] = market_signal_analyzer.analyze_news_batch(
                all_news[:15],
                sector_etf_data=sector_etf_data,
            )
            if result["market_signal"]:
                logger.info(f"Market signal: {result['market_signal'].get('overall_signal')}")
        except Exception as e:
            logger.warning(f"Market signal analysis failed: {e}")

        # 3-3. ê¸´ê¸‰ ë‰´ìŠ¤ ê°ì§€
        try:
            result["breaking_news"] = market_signal_analyzer.detect_breaking_news(all_news)
        except Exception as e:
            logger.warning(f"Breaking news detection failed: {e}")

        # 3-4. ì„¹í„°ë³„ ë¶„ë¥˜
        try:
            result["sector_news"] = market_signal_analyzer.categorize_by_sector(all_news)
        except Exception as e:
            logger.warning(f"Sector categorization failed: {e}")

    # 4. ë¦¬í¬íŠ¸ ì¤‘ìš”ë„ í‰ê°€ - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if reports:
        scored_reports = importance_scorer.score_batch(reports)
        scored_reports.sort(key=lambda x: x.importance_score, reverse=True)
        result["reports"] = scored_reports[:settings.MAX_REPORTS_COUNT]
        logger.info(f"Reports top scores: {[f'{r.title[:20]}({r.importance_score})' for r in result['reports'][:5]]}")

        # AI ìš”ì•½
        try:
            result["reports_summary"] = report_summarizer.summarize_reports(
                result["reports"][:10]
            )
        except Exception as e:
            logger.warning(f"Report summarization failed: {e}")

        # 4-1. ê°œë³„ ë¦¬í¬íŠ¸ AI ë¶„ì„ (PDF ì¶”ì¶œëœ ê²ƒë§Œ)
        pdf_reports = [r for r in result["reports"] if r.extra_data.get("pdf_text")]
        if pdf_reports:
            try:
                report_analyzer.analyze_batch(pdf_reports, max_items=5)
                analyzed_count = sum(1 for r in pdf_reports if r.extra_data.get("ai_analysis"))
                logger.info(f"Analyzed {analyzed_count} reports with AI")
            except Exception as e:
                logger.warning(f"Report analysis failed: {e}")

    # 5. ìœ íŠœë¸Œ ì¤‘ìš”ë„ í‰ê°€ ë° ìš”ì•½ (í•œêµ­) - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if korean_videos:
        scored = importance_scorer.score_batch(korean_videos)
        scored.sort(key=lambda x: x.importance_score, reverse=True)
        result["korean_videos"] = scored[:5]  # í•œêµ­ 5ê°œ
        logger.info(f"Korean YouTube top scores: {[f'{v.title[:20]}({v.importance_score})' for v in result['korean_videos']]}")

        for video in result["korean_videos"]:
            try:
                summary = video_summarizer.summarize_video(video)
                if summary:
                    result["video_summaries"][video.id] = summary
            except Exception as e:
                logger.warning(f"Video summarization failed for {video.title[:30]}: {e}")

    # 6. ìœ íŠœë¸Œ ì¤‘ìš”ë„ í‰ê°€ ë° ìš”ì•½ (í•´ì™¸) - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if intl_videos:
        scored = importance_scorer.score_batch(intl_videos)
        scored.sort(key=lambda x: x.importance_score, reverse=True)
        result["international_videos"] = scored[:5]  # í•´ì™¸ 5ê°œ
        logger.info(f"Intl YouTube top scores: {[f'{v.title[:20]}({v.importance_score})' for v in result['international_videos']]}")

        for video in result["international_videos"]:
            try:
                summary = video_summarizer.summarize_video(video)
                if summary:
                    result["video_summaries"][video.id] = summary
            except Exception as e:
                logger.warning(f"Video summarization failed for {video.title[:30]}: {e}")

    return result


def get_schedule_type() -> tuple[str, str]:
    """
    í˜„ì¬ ì‹¤í–‰ ì‹œê°„ì— ë”°ë¥¸ ìŠ¤ì¼€ì¤„ íƒ€ì… ë°˜í™˜

    Returns:
        (schedule_type, header_title)
    """
    from src.utils.constants import ScheduleSettings
    from src.utils.market_holiday import check_market_holidays

    now = datetime.now()
    weekday = now.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    hour = now.hour

    # ì£¼ë§ ìŠ¤ì¼€ì¤„
    if weekday == 5:  # í† ìš”ì¼
        return ("saturday", ScheduleSettings.SATURDAY_TITLE)
    elif weekday == 6:  # ì¼ìš”ì¼
        return ("sunday", ScheduleSettings.SUNDAY_TITLE)

    # í‰ì¼ íœ´ì¥ì¼ ì²´í¬
    holiday_info = check_market_holidays(now)
    if holiday_info.is_holiday:
        return ("holiday", ScheduleSettings.HOLIDAY_TITLE)

    # í‰ì¼ ìŠ¤ì¼€ì¤„
    if ScheduleSettings.MORNING_START_HOUR <= hour <= ScheduleSettings.MORNING_END_HOUR:
        return ("morning", ScheduleSettings.MORNING_TITLE)
    elif ScheduleSettings.NOON_START_HOUR <= hour <= ScheduleSettings.NOON_END_HOUR:
        return ("noon", ScheduleSettings.NOON_TITLE)
    elif ScheduleSettings.AFTERNOON_START_HOUR <= hour <= ScheduleSettings.AFTERNOON_END_HOUR:
        return ("afternoon", ScheduleSettings.AFTERNOON_TITLE)
    return ("manual", ScheduleSettings.MANUAL_TITLE)


def _build_live_market_history(live_market) -> dict:
    """ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„°ë¥¼ ì´ë ¥ dict í˜•íƒœë¡œ ë³€í™˜ (í´ë°±ìš©)"""
    today_key = datetime.now().strftime("%Y-%m-%d")
    day_data = {}

    for attr, key in [
        ("kospi", "kospi"),
        ("kosdaq", "kosdaq"),
        ("usd_krw", "usd_krw"),
        ("jpy_krw", "jpy_krw"),
        ("eur_krw", "eur_krw"),
        ("wti", "wti"),
        ("gold", "gold"),
    ]:
        data = getattr(live_market, attr, None)
        if data:
            day_data[key] = {
                "value": data.value,
                "change": data.change,
                "change_percent": data.change_percent,
                "is_up": data.is_up,
            }

    return {today_key: day_data} if day_data else {}


def send_weekend_to_discord(analyzed: dict, schedule_type: str) -> bool:
    """ì£¼ë§ ì „ìš© Discord ì „ì†¡ (í† ìš”ì¼: ë¦¬ë·°, ì¼ìš”ì¼: ì „ë§)"""
    from src.analyzer.weekly_summarizer import weekly_summarizer, weekly_preview
    from src.discord import create_weekly_review_embed, create_weekly_preview_embed

    logger.info(f"=== Sending Weekend Content ({schedule_type}) ===")

    now = datetime.now()
    embeds = []

    # ë‰´ìŠ¤ì™€ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    all_news = analyzed.get("korean_news", []) + analyzed.get("international_news", [])
    reports = analyzed.get("reports", [])

    if schedule_type == "saturday":
        # í† ìš”ì¼ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
        today_key = now.strftime("%Y-%m-%d")
        review_cache_id = f"weekly_review_{today_key}"
        if cache.is_sent(review_cache_id, "weekend"):
            logger.info(f"Saturday weekly review already sent today ({today_key}). Skipping.")
            return True

        # í† ìš”ì¼: ì£¼ê°„ ë¦¬ë·° (ì•„ì¹´ì´ë¸Œ ê¸°ë°˜)
        archive_count = weekly_archive.get_items_count()
        logger.info(f"Weekly archive has {archive_count} items")

        if archive_count > 0:
            # ì•„ì¹´ì´ë¸Œì—ì„œ ì´ë²ˆ ì£¼ ë‰´ìŠ¤/ë¦¬í¬íŠ¸ ë¡œë“œ
            archived_news = weekly_archive.get_top_items(max_count=30, content_type="news")
            archived_reports = weekly_archive.get_top_items(max_count=10, content_type="report")
            archived_items = archived_news + archived_reports
            sector_etf_history = weekly_archive.get_sector_etf_history()
            market_index_history = weekly_archive.get_market_index_history()
            weekly_summary_data = weekly_archive.get_weekly_summary()
            logger.info(f"Using archive: {len(archived_news)} news + {len(archived_reports)} reports + {len(sector_etf_history)} days ETF data + {len(market_index_history)} days index data")

            # AI ì •ì„±ì  ë¶„ì„ë§Œ ìš”ì²­
            review_data = weekly_summarizer.generate_weekly_review(
                archived_items=archived_items,
                live_news=all_news[:10],
                sector_etf_history=sector_etf_history,
                market_index_history=market_index_history,
            )

            # Embedì— ëª…ì‹œì  íŒŒë¼ë¯¸í„°ë¡œ ì›ë³¸ ë°ì´í„° ì „ë‹¬
            embeds = create_weekly_review_embed(
                date=now,
                review_data=review_data,
                archived_items=archived_items,
                weekly_summary_data=weekly_summary_data,
                market_index_history=market_index_history,
                sector_etf_history=sector_etf_history,
            )

            # ë¦¬ë·° ìƒì„± í›„ ì•„ì¹´ì´ë¸Œ ë¦¬ì…‹
            weekly_archive.reset()
        else:
            # ì•„ì¹´ì´ë¸Œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤ì‹œê°„ ë‰´ìŠ¤ + ì‹¤ì‹œê°„ ì‹œì„¸ë¡œ í´ë°±
            logger.warning("Weekly archive is empty, falling back to live data")

            live_market_history = {}
            live_sector_history = {}
            try:
                live_market = market_data_collector.collect()
                if live_market and (live_market.kospi or live_market.kosdaq):
                    live_market_history = _build_live_market_history(live_market)
                    logger.info("Collected live market index data for weekly review")
            except Exception as e:
                logger.warning(f"Failed to collect live market data: {e}")

            try:
                live_etf = market_data_collector.collect_sector_etfs()
                if live_etf:
                    today_key = now.strftime("%Y-%m-%d")
                    day_etf = {}
                    for sector, etf in live_etf.items():
                        day_etf[sector] = {
                            "etf_name": etf.etf_name,
                            "price": etf.price,
                            "change": etf.change,
                            "change_percent": etf.change_percent,
                            "is_up": etf.is_up,
                        }
                    live_sector_history = {today_key: day_etf}
            except Exception as e:
                logger.warning(f"Failed to collect live sector ETF data: {e}")

            # AI ì •ì„±ì  ë¶„ì„
            review_data = weekly_summarizer.generate_weekly_review(
                news_items=all_news[:25],
                report_items=reports[:10],
                market_index_history=live_market_history or None,
                sector_etf_history=live_sector_history or None,
            )

            # í´ë°±ìš© weekly_summary_data ê³„ì‚°
            live_weekly_summary = {}
            if live_market_history:
                for key in ["kospi", "kosdaq", "usd_krw", "jpy_krw", "eur_krw", "wti", "gold"]:
                    for day_data in live_market_history.values():
                        data = day_data.get(key)
                        if data:
                            live_weekly_summary[key] = {
                                "start": data["value"],
                                "end": data["value"],
                                "change": 0,
                                "change_pct": 0,
                            }

            embeds = create_weekly_review_embed(
                date=now,
                review_data=review_data,
                archived_items=[],
                weekly_summary_data=live_weekly_summary,
                market_index_history=live_market_history,
                sector_etf_history=live_sector_history,
            )

    elif schedule_type == "sunday":
        # ì¼ìš”ì¼ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
        today_key = now.strftime("%Y-%m-%d")
        preview_cache_id = f"weekly_sunday_{today_key}"
        if cache.is_sent(preview_cache_id, "weekend"):
            logger.info(f"Sunday weekly preview already sent today ({today_key}). Skipping.")
            return True

        # ì¼ìš”ì¼: ì£¼ê°„ ì „ë§
        logger.info("Generating weekly preview...")
        preview_data = weekly_preview.generate_weekly_preview(
            recent_news=all_news[:20],
            recent_reports=reports[:10],
        )
        # ì¶œì²˜ ë§í¬ìš© ë‰´ìŠ¤/ë¦¬í¬íŠ¸ URL ì •ë³´ ì²¨ë¶€
        if preview_data:
            preview_data["_source_items"] = [
                {"title": n.title, "url": n.url, "source": n.source or ""}
                for n in (list(all_news[:20]) + list(reports[:10]))
            ]
        embeds = create_weekly_preview_embed(now, preview_data)

    if not embeds:
        logger.warning("No weekend embeds generated")
        return False

    success = discord_sender.send_multiple_embeds(
        embeds=embeds,
        username="Market Rader ğŸ“ˆ",
    )

    if success:
        logger.info(f"Successfully sent {len(embeds)} weekend embeds to Discord")
        # ì¤‘ë³µ ì „ì†¡ ë°©ì§€ë¥¼ ìœ„í•´ ìºì‹œì— ê¸°ë¡
        today_key = now.strftime("%Y-%m-%d")
        cache.mark_as_sent(f"weekly_{schedule_type}_{today_key}", "weekend")
    else:
        logger.error("Failed to send weekend content to Discord")

    return success


def send_to_discord(analyzed: dict) -> bool:
    """Discordë¡œ ì „ì†¡"""
    from src.utils.constants import NewsSettings, EmbedColors

    logger.info("=== Sending to Discord ===")

    embeds = []
    now = datetime.now()
    schedule_type, header_title = get_schedule_type()

    # ì£¼ë§ ìŠ¤ì¼€ì¤„ ì²˜ë¦¬
    if schedule_type in ("saturday", "sunday"):
        return send_weekend_to_discord(analyzed, schedule_type)

    # ìŠ¤ì¼€ì¤„ íƒ€ì…ì— ë”°ë¥¸ ì½˜í…ì¸  ì„¤ì •
    is_brief_schedule = schedule_type in ("noon", "afternoon")  # ê°„ëµ ìŠ¤ì¼€ì¤„ (ë‰´ìŠ¤ë§Œ)

    if schedule_type == "noon":
        # ì˜¤í›„ 12ì‹œ: í•œêµ­ ë‰´ìŠ¤ ìœ„ì£¼ (ìµœëŒ€ 15ê°œ, ì¤‘ìš”ë„ ìˆœ)
        korean_news = analyzed.get("korean_news", [])[:NewsSettings.NOON_MAX_KOREAN_NEWS]
        intl_news = []  # í•´ì™¸ ë‰´ìŠ¤ ì œì™¸
        logger.info(f"Noon schedule: Korean news only ({len(korean_news)} items)")
    elif schedule_type == "afternoon":
        # ì˜¤í›„ 5ì‹œ: í•œêµ­ ë‰´ìŠ¤ ìœ„ì£¼ (ìµœëŒ€ 15ê°œ, ì¤‘ìš”ë„ ìˆœ) - ë‚®ê³¼ ë™ì¼
        korean_news = analyzed.get("korean_news", [])[:NewsSettings.AFTERNOON_MAX_KOREAN_NEWS]
        intl_news = []  # í•´ì™¸ ë‰´ìŠ¤ ì œì™¸
        logger.info(f"Afternoon schedule: Korean news only ({len(korean_news)} items)")
    else:
        # ì˜¤ì „ 7ì‹œ/ìˆ˜ë™: ì „ì²´ ì½˜í…ì¸ 
        korean_news = analyzed.get("korean_news", [])[:NewsSettings.MAX_KOREAN_NEWS]
        intl_news = analyzed.get("international_news", [])[:NewsSettings.MAX_INTL_NEWS]

    all_news = korean_news + intl_news

    # 0. Morning Brief (ì˜¤ì „ ìŠ¤ì¼€ì¤„ì—ì„œë§Œ)
    morning_briefs = analyzed.get("morning_briefs", [])
    if schedule_type == "morning" and morning_briefs:
        # Morning Brief ì¢…í•© ìš”ì•½ ìƒì„±
        combined_summary = analyzed.get("morning_brief_summary")
        brief_embeds = create_morning_brief_embed(morning_briefs, combined_summary)
        embeds.extend(brief_embeds)
        logger.info(f"Added {len(brief_embeds)} Morning Brief embeds")

    # 0-1. AI ì•„ì¹¨ ì „ëµ ë¸Œë¦¬í•‘ (ì˜¤ì „ ìŠ¤ì¼€ì¤„ì—ì„œë§Œ)
    reports = analyzed.get("reports", [])
    if schedule_type == "morning" and all_news:
        try:
            morning_briefing = market_briefing_generator.generate_morning_strategy(
                news_items=all_news[:10],
                morning_briefs=morning_briefs,
                report_items=reports[:5],
            )
            if morning_briefing:
                # ë¸Œë¦¬í•‘ ê²€ì¦
                briefing_text = briefing_validator.get_briefing_text(morning_briefing)
                validation = briefing_validator.validate_briefing(
                    briefing_text=briefing_text,
                    market_data=None,  # ì•„ì¹¨ì—” ì‹œì¥ ë°ì´í„° ì—†ìŒ
                    news_items=all_news[:10],
                    report_items=reports[:5],
                )

                if validation.is_valid:
                    strategy_embed = create_morning_strategy_embed(morning_briefing, now)
                    embeds.append(strategy_embed)
                    logger.info(f"Added AI morning strategy briefing (validation score: {validation.score})")
                else:
                    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ë‚¨ê¸°ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
                    logger.warning(f"Morning strategy validation FAILED: {validation.errors}")
                    logger.warning(f"Validation warnings: {validation.warnings}")
        except Exception as e:
            logger.warning(f"Failed to generate morning strategy: {e}")

    # 0-2. ì¥ ë§ˆê° ì‹œí™© ë° AI ë¦¬ë·° (ì˜¤í›„ 5ì‹œ ìŠ¤ì¼€ì¤„ì—ì„œë§Œ)
    if schedule_type == "afternoon":
        # ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
        market_data = None
        try:
            market_data = market_data_collector.collect()
            if market_data.kospi or market_data.usd_krw:
                market_close_embed = create_market_close_embed(market_data, now)
                embeds.append(market_close_embed)
                logger.info("Added market close summary embed")
        except Exception as e:
            logger.warning(f"Failed to collect market data: {e}")

        # AI ì¥ ë§ˆê° ë¦¬ë·° ìƒì„± (ì •ì„±ì  ë¶„ì„ë§Œ - ìˆ˜ì¹˜ëŠ” market_close_embedì—ì„œ ì§ì ‘ í‘œì‹œ)
        if all_news:
            try:
                closing_briefing = market_briefing_generator.generate_closing_review(
                    news_items=all_news[:10],
                    report_items=reports[:5] if reports else None,
                )
                if closing_briefing:
                    # ë¸Œë¦¬í•‘ ê²€ì¦ (í‚¤ì›Œë“œ/ì¶œì²˜ë§Œ - ìˆ˜ì¹˜ ê²€ì¦ ë¶ˆí•„ìš”)
                    briefing_text = briefing_validator.get_briefing_text(closing_briefing)
                    validation = briefing_validator.validate_briefing(
                        briefing_text=briefing_text,
                        news_items=all_news[:10],
                        report_items=reports[:5] if reports else None,
                    )

                    if validation.is_valid:
                        review_embed = create_closing_review_embed(closing_briefing, now)
                        embeds.append(review_embed)
                        logger.info(f"Added AI closing review briefing (validation score: {validation.score})")
                    else:
                        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ë‚¨ê¸°ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
                        logger.warning(f"Closing review validation FAILED: {validation.errors}")
                        logger.warning(f"Validation warnings: {validation.warnings}")
            except Exception as e:
                logger.warning(f"Failed to generate closing review: {e}")

    # 0-3. ê¸´ê¸‰ ë‰´ìŠ¤ (ìˆëŠ” ê²½ìš° ìµœìƒë‹¨)
    breaking_news = analyzed.get("breaking_news", [])
    if breaking_news:
        breaking_embed = create_breaking_news_embed(breaking_news)
        if breaking_embed:
            embeds.append(breaking_embed)
            logger.info(f"Added breaking news embed ({len(breaking_news)} items)")

    # 1. ì‹œì¥ ì‹œê·¸ë„ í—¤ë” (AI ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´) ë˜ëŠ” ì¼ë°˜ í—¤ë”
    market_signal = analyzed.get("market_signal")
    if all_news:
        if market_signal:
            # ì‹œì¥ ì‹œê·¸ë„ Embed ì‚¬ìš©
            signal_embed = create_market_signal_embed(
                date=now,
                signal_data=market_signal,
                news_count=len(all_news),
                title_override=header_title,
            )
            embeds.append(signal_embed)
        else:
            # ê¸°ì¡´ í—¤ë” Embed
            header_embed = create_news_header_embed(
                date=now,
                news_count=len(all_news),
                summary=analyzed.get("news_summary"),
                title_override=header_title,
            )
            embeds.append(header_embed)

    # 2. êµ­ë‚´ ë‰´ìŠ¤
    if korean_news:
        korean_embeds = create_news_list_embeds(
            items=korean_news,
            title=f"ğŸ‡°ğŸ‡· êµ­ë‚´ ë‰´ìŠ¤ ({len(korean_news)}ê±´)",
            items_per_embed=5,
            color=EmbedColors.NEWS_KOREAN,
        )
        embeds.extend(korean_embeds)

    # 3. í•´ì™¸ ë‰´ìŠ¤ (ê°„ëµ ìŠ¤ì¼€ì¤„ì—ì„œëŠ” ê±´ë„ˆëœ€)
    if intl_news and not is_brief_schedule:
        intl_embeds = create_news_list_embeds(
            items=intl_news,
            title=f"ğŸ‡ºğŸ‡¸ í•´ì™¸ ë‰´ìŠ¤ ({len(intl_news)}ê±´)",
            items_per_embed=5,
            color=EmbedColors.NEWS_INTL,
        )
        embeds.extend(intl_embeds)

    # ê°„ëµ ìŠ¤ì¼€ì¤„(ì ì‹¬/ì˜¤í›„)ì—ì„œëŠ” ë¦¬í¬íŠ¸ì™€ ìœ íŠœë¸Œ ì œì™¸
    reports = []
    korean_videos = []
    intl_videos = []
    video_summaries = {}

    if not is_brief_schedule:
        # 4. ë¦¬í¬íŠ¸ (AI ë¶„ì„ í¬í•¨ ì‹œ ìƒì„¸ Embed ì‚¬ìš©)
        reports = analyzed.get("reports", [])[:NewsSettings.MAX_REPORTS]
        if reports:
            # AI ë¶„ì„ëœ ë¦¬í¬íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            analyzed_reports = [r for r in reports if r.extra_data.get("ai_analysis")]

            if analyzed_reports:
                # AI ë¶„ì„ í¬í•¨ ìƒì„¸ Embed ì‚¬ìš©
                report_embeds = create_reports_with_analysis_embeds(
                    items=reports,
                    max_detailed=3,  # ìƒì„¸ ë¶„ì„ 3ê°œ
                    max_list=7,  # ë‚˜ë¨¸ì§€ ëª©ë¡ 7ê°œ
                )
                embeds.extend(report_embeds)
                logger.info(f"Added {len(report_embeds)} report embeds (with {len(analyzed_reports)} analyzed)")
            else:
                # ê¸°ì¡´ ë°©ì‹ (AI ë¶„ì„ ì—†ì„ ë•Œ)
                reports_header = create_reports_header_embed(
                    report_count=len(reports),
                    summary=analyzed.get("reports_summary"),
                )
                embeds.append(reports_header)

                reports_list = create_reports_list_embed(
                    items=reports,
                    max_items=10,
                )
                embeds.append(reports_list)

        # 5. í•œêµ­ ìœ íŠœë¸Œ
        korean_videos = analyzed.get("korean_videos", [])[:NewsSettings.MAX_YOUTUBE_KOREAN]
        video_summaries = analyzed.get("video_summaries", {})

        if korean_videos:
            korean_yt_list = create_youtube_list_embed(
                items=korean_videos,
                title=f"ğŸ‡°ğŸ‡· í•œêµ­ ìœ íŠœë¸Œ ({len(korean_videos)}ê±´)",
                max_items=5,
                video_summaries=video_summaries,
            )
            embeds.append(korean_yt_list)

        # 6. í•´ì™¸ ìœ íŠœë¸Œ
        intl_videos = analyzed.get("international_videos", [])[:NewsSettings.MAX_YOUTUBE_INTL]

        if intl_videos:
            intl_yt_list = create_youtube_list_embed(
                items=intl_videos,
                title=f"ğŸ‡ºğŸ‡¸ í•´ì™¸ ìœ íŠœë¸Œ ({len(intl_videos)}ê±´)",
                max_items=5,
                video_summaries=video_summaries,
            )
            embeds.append(intl_yt_list)

    all_videos = korean_videos + intl_videos

    # Discordë¡œ ì „ì†¡
    if not embeds:
        logger.info("No content to send")
        return True

    success = discord_sender.send_multiple_embeds(
        embeds=embeds,
        username="Market Rader ğŸ“ˆ",
    )

    if success:
        # ìºì‹œì— ì „ì†¡ëœ í•­ëª© ê¸°ë¡
        cache.mark_multiple_as_sent([n.id for n in all_news], "news")
        cache.mark_multiple_as_sent([r.id for r in reports], "reports")
        cache.mark_multiple_as_sent([v.id for v in all_videos], "youtube")
        if morning_briefs:
            cache.mark_multiple_as_sent([b.id for b in morning_briefs], "morning_brief")
        logger.info(f"Successfully sent {len(embeds)} embeds to Discord")

        # ì£¼ê°„ ì•„ì¹´ì´ë¸Œì— ì €ì¥ (í‰ì¼ë§Œ, ì¤‘ìš”ë„ 0.3 ì´ìƒ)
        if schedule_type not in ("saturday", "sunday"):
            archive_candidates = [
                item for item in (list(all_news) + list(reports))
                if item.importance_score >= 0.3
            ]
            if archive_candidates:
                added = weekly_archive.add_items(archive_candidates)
                logger.info(f"Archived {added} items for weekly review")

            # ì„¹í„° ETF ì‹œì„¸ë„ ì•„ì¹´ì´ë¸Œì— ì €ì¥
            sector_etf_data = analyzed.get("sector_etf_data", {})
            if sector_etf_data:
                weekly_archive.add_sector_etf_data(sector_etf_data)

            # ì‹œì¥ ì§€ìˆ˜(ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥/í™˜ìœ¨)ë„ ì•„ì¹´ì´ë¸Œì— ì €ì¥
            market_data = analyzed.get("market_data")
            if market_data:
                weekly_archive.add_market_index_data(market_data)

    else:
        logger.error("Failed to send to Discord")

    return success


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 50)
    logger.info("Market Rader Starting...")
    logger.info(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 50)

    # ì„¤ì • ê²€ì¦
    if not validate_settings():
        logger.error("Configuration validation failed. Exiting.")
        sys.exit(1)

    try:
        # í˜„ì¬ ìŠ¤ì¼€ì¤„ íƒ€ì… í™•ì¸
        schedule_type, _ = get_schedule_type()

        # íœ´ì¥ì¼ ì²˜ë¦¬: ì•ˆë‚´ Embedë§Œ ì „ì†¡í•˜ê³  ì¢…ë£Œ
        if schedule_type == "holiday":
            from src.utils.market_holiday import check_market_holidays
            from src.discord.embeds.holiday_embed import create_holiday_embed

            now = datetime.now()
            holiday_info = check_market_holidays(now)
            logger.info(f"Market holiday detected: {holiday_info.summary}")

            embed = create_holiday_embed(holiday_info, now)
            success = discord_sender.send_multiple_embeds(
                embeds=[embed],
                username="Market Rader ğŸ“ˆ",
            )
            if success:
                logger.info("Holiday notice sent to Discord")
            else:
                logger.error("Failed to send holiday notice")
            return

        is_morning = schedule_type == "morning"

        # 1. ì½˜í…ì¸  ìˆ˜ì§‘ (ë³‘ë ¬ ì‹¤í–‰)
        logger.info("=== Starting Parallel Collection ===")
        news = {"korean": [], "international": []}
        reports = []
        videos = {"korean": [], "international": []}
        morning_briefs = []

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(collect_news): "news",
                # ì˜¤ì „ ìŠ¤ì¼€ì¤„ì—ì„œë§Œ PDF ì¶”ì¶œ (AI ë¶„ì„ìš©)
                executor.submit(collect_reports, extract_pdf=is_morning): "reports",
                executor.submit(collect_youtube): "youtube",
            }

            # ì˜¤ì „ ìŠ¤ì¼€ì¤„ì—ë§Œ Morning Brief ìˆ˜ì§‘
            if is_morning:
                futures[executor.submit(collect_morning_briefs)] = "morning_briefs"

            for future in as_completed(futures):
                task_name = futures[future]
                try:
                    result = future.result()
                    if task_name == "news":
                        news = result
                    elif task_name == "reports":
                        reports = result
                    elif task_name == "youtube":
                        videos = result
                    elif task_name == "morning_briefs":
                        morning_briefs = result
                    logger.info(f"Completed: {task_name}")
                except Exception as e:
                    logger.error(f"Failed to collect {task_name}: {e}")

        # ìˆ˜ì§‘ëœ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        all_news = news.get("korean", []) + news.get("international", [])
        if not all_news and not reports and not videos and not morning_briefs:
            logger.info("No new content collected. Exiting.")
            return

        # 2. ë¶„ì„ ë° ìš”ì•½
        analyzed = analyze_content(news, reports, videos, morning_briefs)

        # 3. Discord ì „ì†¡
        success = send_to_discord(analyzed)

        # 4. ìºì‹œ ì •ë¦¬
        cache.cleanup_old_entries(days=7)

        if success:
            logger.info("Market Rader completed successfully!")
        else:
            logger.warning("Market Rader completed with some errors")

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise


if __name__ == "__main__":
    main()
