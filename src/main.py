"""
Market Rader - ì£¼ì‹ ë‰´ìŠ¤ ë””ìŠ¤ì½”ë“œ ë´‡
ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from config.settings import settings, get_news_sources, get_youtube_channels
from src.utils.logger import logger
from src.utils.cache import cache

# Collectors
from src.collectors.base import ContentItem, ContentType
from src.collectors.news import (
    NaverFinanceNewsCollector,
    RSSNewsCollector,
    create_rss_collectors,
    InvestingNewsCollector,
)
from src.collectors.reports import NaverResearchCollector, SeekingAlphaCollector
from src.collectors.youtube import YouTubeChannelMonitor, transcript_extractor

# Analyzers
from src.analyzer import (
    news_summarizer,
    report_summarizer,
    video_summarizer,
    importance_scorer,
)

# Discord
from src.discord import (
    discord_sender,
    create_news_header_embed,
    create_news_list_embeds,
    create_reports_header_embed,
    create_reports_list_embed,
    create_youtube_header_embed,
    create_youtube_list_embed,
)


def validate_settings() -> bool:
    """ì„¤ì • ê²€ì¦"""
    errors = settings.validate()
    if errors:
        for error in errors:
            logger.error(f"Configuration error: {error}")
        return False
    return True


def collect_news() -> dict:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ (êµ­ë‚´/í•´ì™¸ ë¶„ë¦¬, ë³‘ë ¬ ì²˜ë¦¬)"""
    from src.utils.constants import get_priority_from_string

    logger.info("=== Collecting News (Parallel) ===")
    korean_news = []
    international_news = []

    # ìˆ˜ì§‘ íƒœìŠ¤í¬ ì •ì˜
    def collect_naver():
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤"""
        collector = NaverFinanceNewsCollector(categories=["stock", "economy"])
        items = collector.collect()
        for item in items:
            item.extra_data["region"] = "korean"
        return ("korean", items, "Naver Finance")

    def collect_investing():
        """ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ì¸ê¸° ë‰´ìŠ¤ (í˜„ì¬ ì°¨ë‹¨ë¨ - ë¹„í™œì„±í™”)"""
        # ì¸ë² ìŠ¤íŒ…ë‹·ì»´ì´ ë´‡ ì°¨ë‹¨ ì¤‘ì´ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        # TODO: ë‹¤ë¥¸ ì¸ê¸° ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œ ëŒ€ì²´ í•„ìš”
        return ("korean", [], "Investing.com (disabled)")

    def collect_rss(source: dict, region: str):
        """RSS ë‰´ìŠ¤ ìˆ˜ì§‘"""
        priority = get_priority_from_string(source.get("priority", "medium"))
        collector = RSSNewsCollector(
            name=source["name"],
            url=source["url"],
            priority=priority,
        )
        items = collector.collect()
        for item in items:
            item.extra_data["region"] = region
        return (region, items, source["name"])

    # RSS ì†ŒìŠ¤ ë¡œë“œ
    news_config = get_news_sources()
    korean_sources = news_config.get("news", {}).get("korean", [])
    intl_sources = news_config.get("news", {}).get("international", [])

    # ëª¨ë“  ìˆ˜ì§‘ íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = []

        # ê¸°ë³¸ ìˆ˜ì§‘ê¸°
        futures.append(executor.submit(collect_naver))
        futures.append(executor.submit(collect_investing))

        # RSS ìˆ˜ì§‘ê¸°ë“¤
        for source in korean_sources:
            if source.get("type") == "rss" and source.get("enabled", True):
                futures.append(executor.submit(collect_rss, source, "korean"))

        for source in intl_sources:
            if source.get("type") == "rss" and source.get("enabled", True):
                futures.append(executor.submit(collect_rss, source, "international"))

        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(futures):
            try:
                region, items, source_name = future.result()
                if region == "korean":
                    korean_news.extend(items)
                else:
                    international_news.extend(items)
                logger.info(f"{source_name}: {len(items)} items")
            except Exception as e:
                logger.error(f"News collection failed: {e}")

    # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
    def dedupe(news_list):
        seen_ids = set()
        unique = []
        for item in news_list:
            if item.id not in seen_ids and not cache.is_sent(item.id, "news"):
                seen_ids.add(item.id)
                unique.append(item)
        return unique

    korean_news = dedupe(korean_news)
    international_news = dedupe(international_news)

    logger.info(f"Korean news: {len(korean_news)}, International: {len(international_news)}")
    return {"korean": korean_news, "international": international_news}


def collect_reports() -> list[ContentItem]:
    """ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
    logger.info("=== Collecting Reports (Parallel) ===")
    all_reports = []

    def collect_naver_research():
        """ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜"""
        collector = NaverResearchCollector(categories=["invest", "company", "market"])
        return collector.collect()

    def collect_seeking_alpha():
        """Seeking Alpha"""
        collector = SeekingAlphaCollector()
        return collector.collect()

    # ë³‘ë ¬ ìˆ˜ì§‘
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = {
            executor.submit(collect_naver_research): "Naver Research",
            executor.submit(collect_seeking_alpha): "Seeking Alpha",
        }

        for future in as_completed(futures):
            source_name = futures[future]
            try:
                reports = future.result()
                all_reports.extend(reports)
                logger.info(f"{source_name}: {len(reports)} items")
            except Exception as e:
                logger.error(f"{source_name} collection failed: {e}")

    # ì¤‘ë³µ ì œê±°
    seen_ids = set()
    unique_reports = []
    for item in all_reports:
        if item.id not in seen_ids and not cache.is_sent(item.id, "reports"):
            seen_ids.add(item.id)
            unique_reports.append(item)

    logger.info(f"Total unique reports: {len(unique_reports)}")
    return unique_reports


def collect_youtube() -> dict:
    """ìœ íŠœë¸Œ ì˜ìƒ ìˆ˜ì§‘ (í•œêµ­/í•´ì™¸ ë¶„ë¦¬)"""
    logger.info("=== Collecting YouTube Videos ===")

    try:
        youtube_monitor = YouTubeChannelMonitor()
        videos = youtube_monitor.collect()

        # ì¤‘ë³µ ì œê±° (ì´ë¯¸ ì „ì†¡ëœ ì˜ìƒ ì œì™¸)
        korean_videos = [
            v for v in videos.get("korean", [])
            if not cache.is_sent(v.id, "youtube")
        ]
        intl_videos = [
            v for v in videos.get("international", [])
            if not cache.is_sent(v.id, "youtube")
        ]

        logger.info(f"Total new videos - Korean: {len(korean_videos)}, Intl: {len(intl_videos)}")
        return {"korean": korean_videos, "international": intl_videos}

    except Exception as e:
        logger.error(f"YouTube collection failed: {e}")
        return {"korean": [], "international": []}


def analyze_content(
    news: dict,
    reports: list[ContentItem],
    videos: dict,
) -> dict:
    """ì½˜í…ì¸  ë¶„ì„ ë° ìš”ì•½"""
    logger.info("=== Analyzing Content ===")

    korean_news = news.get("korean", [])
    intl_news = news.get("international", [])
    korean_videos = videos.get("korean", [])
    intl_videos = videos.get("international", [])

    result = {
        "korean_news": korean_news,
        "international_news": intl_news,
        "news_summary": None,
        "reports": reports,
        "reports_summary": None,
        "korean_videos": korean_videos,
        "international_videos": intl_videos,
        "video_summaries": {},
    }

    # 1. êµ­ë‚´ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
    if korean_news:
        scored = importance_scorer.filter_by_importance(korean_news, min_score=0.3)
        result["korean_news"] = scored[:settings.MAX_NEWS_COUNT]

    # 2. í•´ì™¸ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
    if intl_news:
        scored = importance_scorer.filter_by_importance(intl_news, min_score=0.3)
        result["international_news"] = scored[:settings.MAX_NEWS_COUNT]

    # 3. AI ìš”ì•½ (êµ­ë‚´ + í•´ì™¸ í•©ì³ì„œ)
    all_news = result["korean_news"] + result["international_news"]
    if all_news:
        try:
            result["news_summary"] = news_summarizer.summarize_news_batch(all_news[:15])
        except Exception as e:
            logger.warning(f"News summarization failed: {e}")

    # 4. ë¦¬í¬íŠ¸ ì¤‘ìš”ë„ í‰ê°€ - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if reports:
        scored_reports = importance_scorer.score_batch(reports)
        scored_reports.sort(key=lambda x: x.importance_score, reverse=True)
        result["reports"] = scored_reports[:settings.MAX_REPORTS_COUNT]
        logger.info(f"Reports top scores: {[f'{r.title[:20]}({r.importance_score})' for r in result['reports'][:5]]}")

        # AI ìš”ì•½
        try:
            result["reports_summary"] = report_summarizer.summarize_reports(
                result["reports"][:10]
            )
        except Exception as e:
            logger.warning(f"Report summarization failed: {e}")

    # 5. ìœ íŠœë¸Œ ì¤‘ìš”ë„ í‰ê°€ ë° ìš”ì•½ (í•œêµ­) - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if korean_videos:
        scored = importance_scorer.score_batch(korean_videos)
        scored.sort(key=lambda x: x.importance_score, reverse=True)
        result["korean_videos"] = scored[:5]  # í•œêµ­ 5ê°œ
        logger.info(f"Korean YouTube top scores: {[f'{v.title[:20]}({v.importance_score})' for v in result['korean_videos']]}")

        for video in result["korean_videos"]:
            try:
                summary = video_summarizer.summarize_video(video)
                if summary:
                    result["video_summaries"][video.id] = summary
            except Exception as e:
                logger.warning(f"Video summarization failed for {video.title[:30]}: {e}")

    # 6. ìœ íŠœë¸Œ ì¤‘ìš”ë„ í‰ê°€ ë° ìš”ì•½ (í•´ì™¸) - ì¤‘ìš”ë„ ë†’ì€ ìˆœ
    if intl_videos:
        scored = importance_scorer.score_batch(intl_videos)
        scored.sort(key=lambda x: x.importance_score, reverse=True)
        result["international_videos"] = scored[:5]  # í•´ì™¸ 5ê°œ
        logger.info(f"Intl YouTube top scores: {[f'{v.title[:20]}({v.importance_score})' for v in result['international_videos']]}")

        for video in result["international_videos"]:
            try:
                summary = video_summarizer.summarize_video(video)
                if summary:
                    result["video_summaries"][video.id] = summary
            except Exception as e:
                logger.warning(f"Video summarization failed for {video.title[:30]}: {e}")

    return result


def get_schedule_type() -> tuple[str, str]:
    """
    í˜„ì¬ ì‹¤í–‰ ì‹œê°„ì— ë”°ë¥¸ ìŠ¤ì¼€ì¤„ íƒ€ì… ë°˜í™˜

    Returns:
        (schedule_type, header_title)
    """
    from src.utils.constants import ScheduleSettings

    hour = datetime.now().hour
    if ScheduleSettings.MORNING_START_HOUR <= hour <= ScheduleSettings.MORNING_END_HOUR:
        return ("morning", ScheduleSettings.MORNING_TITLE)
    elif ScheduleSettings.NOON_START_HOUR <= hour <= ScheduleSettings.NOON_END_HOUR:
        return ("noon", ScheduleSettings.NOON_TITLE)
    return ("manual", ScheduleSettings.MANUAL_TITLE)


def send_to_discord(analyzed: dict) -> bool:
    """Discordë¡œ ì „ì†¡"""
    from src.utils.constants import NewsSettings, EmbedColors

    logger.info("=== Sending to Discord ===")

    embeds = []
    now = datetime.now()
    schedule_type, header_title = get_schedule_type()

    # ìŠ¤ì¼€ì¤„ íƒ€ì…ì— ë”°ë¥¸ ì½˜í…ì¸  ì„¤ì •
    is_noon = schedule_type == "noon"

    if is_noon:
        # ì˜¤í›„ 12ì‹œ: í•œêµ­ ë‰´ìŠ¤ ìœ„ì£¼ (ìµœëŒ€ 15ê°œ, ì¤‘ìš”ë„ ìˆœ)
        korean_news = analyzed.get("korean_news", [])[:NewsSettings.NOON_MAX_KOREAN_NEWS]
        intl_news = []  # í•´ì™¸ ë‰´ìŠ¤ ì œì™¸
        logger.info(f"Noon schedule: Korean news only ({len(korean_news)} items)")
    else:
        # ì˜¤ì „ 7ì‹œ/ìˆ˜ë™: ì „ì²´ ì½˜í…ì¸ 
        korean_news = analyzed.get("korean_news", [])[:NewsSettings.MAX_KOREAN_NEWS]
        intl_news = analyzed.get("international_news", [])[:NewsSettings.MAX_INTL_NEWS]

    all_news = korean_news + intl_news

    # 1. í—¤ë” (AI ìš”ì•½)
    if all_news:
        header_embed = create_news_header_embed(
            date=now,
            news_count=len(all_news),
            summary=analyzed.get("news_summary"),
            title_override=header_title,
        )
        embeds.append(header_embed)

    # 2. êµ­ë‚´ ë‰´ìŠ¤
    if korean_news:
        korean_embeds = create_news_list_embeds(
            items=korean_news,
            title=f"ğŸ‡°ğŸ‡· êµ­ë‚´ ë‰´ìŠ¤ ({len(korean_news)}ê±´)",
            items_per_embed=5,
            color=EmbedColors.NEWS_KOREAN,
        )
        embeds.extend(korean_embeds)

    # 3. í•´ì™¸ ë‰´ìŠ¤ (ì ì‹¬ ìŠ¤ì¼€ì¤„ì—ì„œëŠ” ê±´ë„ˆëœ€)
    if intl_news and not is_noon:
        intl_embeds = create_news_list_embeds(
            items=intl_news,
            title=f"ğŸ‡ºğŸ‡¸ í•´ì™¸ ë‰´ìŠ¤ ({len(intl_news)}ê±´)",
            items_per_embed=5,
            color=EmbedColors.NEWS_INTL,
        )
        embeds.extend(intl_embeds)

    # ì ì‹¬ ìŠ¤ì¼€ì¤„ì—ì„œëŠ” ë¦¬í¬íŠ¸ì™€ ìœ íŠœë¸Œ ì œì™¸
    reports = []
    korean_videos = []
    intl_videos = []
    video_summaries = {}

    if not is_noon:
        # 4. ë¦¬í¬íŠ¸
        reports = analyzed.get("reports", [])[:NewsSettings.MAX_REPORTS]
        if reports:
            reports_header = create_reports_header_embed(
                report_count=len(reports),
                summary=analyzed.get("reports_summary"),
            )
            embeds.append(reports_header)

            reports_list = create_reports_list_embed(
                items=reports,
                max_items=10,
            )
            embeds.append(reports_list)

        # 5. í•œêµ­ ìœ íŠœë¸Œ
        korean_videos = analyzed.get("korean_videos", [])[:NewsSettings.MAX_YOUTUBE_KOREAN]
        video_summaries = analyzed.get("video_summaries", {})

        if korean_videos:
            korean_yt_list = create_youtube_list_embed(
                items=korean_videos,
                title=f"ğŸ‡°ğŸ‡· í•œêµ­ ìœ íŠœë¸Œ ({len(korean_videos)}ê±´)",
                max_items=5,
                video_summaries=video_summaries,
            )
            embeds.append(korean_yt_list)

        # 6. í•´ì™¸ ìœ íŠœë¸Œ
        intl_videos = analyzed.get("international_videos", [])[:NewsSettings.MAX_YOUTUBE_INTL]

        if intl_videos:
            intl_yt_list = create_youtube_list_embed(
                items=intl_videos,
                title=f"ğŸ‡ºğŸ‡¸ í•´ì™¸ ìœ íŠœë¸Œ ({len(intl_videos)}ê±´)",
                max_items=5,
                video_summaries=video_summaries,
            )
            embeds.append(intl_yt_list)

    all_videos = korean_videos + intl_videos

    # Discordë¡œ ì „ì†¡
    if not embeds:
        logger.info("No content to send")
        return True

    success = discord_sender.send_multiple_embeds(
        embeds=embeds,
        username="Market Rader ğŸ“ˆ",
    )

    if success:
        # ìºì‹œì— ì „ì†¡ëœ í•­ëª© ê¸°ë¡
        cache.mark_multiple_as_sent([n.id for n in all_news], "news")
        cache.mark_multiple_as_sent([r.id for r in reports], "reports")
        cache.mark_multiple_as_sent([v.id for v in all_videos], "youtube")
        logger.info(f"Successfully sent {len(embeds)} embeds to Discord")
    else:
        logger.error("Failed to send to Discord")

    return success


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 50)
    logger.info("Market Rader Starting...")
    logger.info(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 50)

    # ì„¤ì • ê²€ì¦
    if not validate_settings():
        logger.error("Configuration validation failed. Exiting.")
        sys.exit(1)

    try:
        # 1. ì½˜í…ì¸  ìˆ˜ì§‘ (ë³‘ë ¬ ì‹¤í–‰)
        logger.info("=== Starting Parallel Collection ===")
        news = {"korean": [], "international": []}
        reports = []
        videos = {"korean": [], "international": []}

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {
                executor.submit(collect_news): "news",
                executor.submit(collect_reports): "reports",
                executor.submit(collect_youtube): "youtube",
            }

            for future in as_completed(futures):
                task_name = futures[future]
                try:
                    result = future.result()
                    if task_name == "news":
                        news = result
                    elif task_name == "reports":
                        reports = result
                    elif task_name == "youtube":
                        videos = result
                    logger.info(f"Completed: {task_name}")
                except Exception as e:
                    logger.error(f"Failed to collect {task_name}: {e}")

        # ìˆ˜ì§‘ëœ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        all_news = news.get("korean", []) + news.get("international", [])
        if not all_news and not reports and not videos:
            logger.info("No new content collected. Exiting.")
            return

        # 2. ë¶„ì„ ë° ìš”ì•½
        analyzed = analyze_content(news, reports, videos)

        # 3. Discord ì „ì†¡
        success = send_to_discord(analyzed)

        # 4. ìºì‹œ ì •ë¦¬
        cache.cleanup_old_entries(days=7)

        if success:
            logger.info("Market Rader completed successfully!")
        else:
            logger.warning("Market Rader completed with some errors")

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise


if __name__ == "__main__":
    main()
